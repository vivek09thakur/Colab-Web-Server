{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOcpwnxe3V4yeJZqobcOZEG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivek09thakur/Colab-Web-Server/blob/main/Colab%20Notebook/colab_web_server.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "!./ngrok authtoken 2c2CD7vXkTJYULz1dDGFSViTqY9_6KDavnrbYSNxp3TPJReYa # you token here\n",
        "!nohup python -m flask run &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8xxfWOfBkyv",
        "outputId": "4a9c9d6f-fe7c-437c-ec6e-5cd0b9713549"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-02 17:09:38--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 75.2.60.68, 35.71.179.82, 13.248.244.96, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|75.2.60.68|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13921656 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.28M  16.7MB/s    in 0.8s    \n",
            "\n",
            "2025-03-02 17:09:39 (16.7 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13921656/13921656]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n",
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mistral-inference"
      ],
      "metadata": {
        "id": "rWfKw_7gzaSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-Instruct-v0.3.tar\n",
        "!DIR=$HOME/mistral_7b_instruct_v3 && mkdir -p $DIR && tar -xf mistral-7B-Instruct-v0.3.tar -C $DIR\n",
        "!ls mistral_7b_instruct_v3"
      ],
      "metadata": {
        "id": "HKKlxduxzjcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from mistral_inference.transformer import Transformer\n",
        "from mistral_inference.generate import generate\n",
        "\n",
        "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
        "from mistral_common.protocol.instruct.messages import UserMessage\n",
        "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
        "\n",
        "# load tokenizer\n",
        "mistral_tokenizer = MistralTokenizer.from_file(os.path.expanduser(\"~\")+\"/mistral_7b_instruct_v3/tokenizer.model.v3\")\n",
        "# chat completion request\n",
        "completion_request = ChatCompletionRequest(messages=[UserMessage(content=\"Explain Machine Learning to me in a nutshell.\")])\n",
        "# encode message\n",
        "tokens = mistral_tokenizer.encode_chat_completion(completion_request).tokens\n",
        "# load model\n",
        "model = Transformer.from_folder(os.path.expanduser(\"~\")+\"/mistral_7b_instruct_v3\")\n",
        "# generate results\n",
        "out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=mistral_tokenizer.instruct_tokenizer.tokenizer.eos_id)\n",
        "# decode generated tokens\n",
        "result = mistral_tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n",
        "print(result)"
      ],
      "metadata": {
        "id": "K2plzVpYzuQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mistral_common.protocol.instruct.messages import UserMessage\n",
        "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
        "from mistral_common.protocol.instruct.tool_calls import Function, Tool\n",
        "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
        "from mistral_inference.generate import generate\n",
        "\n",
        "def generate_response(self,prompt):\n",
        "    completion_request = ChatCompletionRequest(\n",
        "        tools=[\n",
        "            Tool(\n",
        "                function=Function(\n",
        "                    name=\"get_current_weather\",\n",
        "                    description=\"Get the current weather\",\n",
        "                    parameters={\n",
        "                        \"type\": \"object\",\n",
        "                        \"properties\": {\n",
        "                            \"location\": {\n",
        "                                \"type\": \"string\",\n",
        "                                \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
        "                                },\n",
        "                            \"format\": {\n",
        "                                \"type\": \"string\",\n",
        "                                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
        "                                \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
        "                            },\n",
        "                        },\n",
        "                        \"required\": [\"location\", \"format\"],\n",
        "                    },\n",
        "                )\n",
        "                )\n",
        "        ],\n",
        "        messages=[UserMessage(content=prompt),],\n",
        "    )\n",
        "    tokens = mistral_tokenizer.encode_chat_completion(completion_request).tokens\n",
        "    out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=mistral_tokenizer.instruct_tokenizer.tokenizer.eos_id)\n",
        "    result = mistral_tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens)[0]\n",
        "    return result\n",
        ""
      ],
      "metadata": {
        "id": "9W1nPlF6zvpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask,request\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return \"<h1>Hello, World!</h1>\"\n",
        "\n",
        "@app.route('/receive_data', methods=['POST'])\n",
        "def receive_data():\n",
        "    data = request.get_json()\n",
        "    response = data['key'] + \" response from google colab \"\n",
        "    return response, 200"
      ],
      "metadata": {
        "id": "SCOS7UNz_AcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmlIgvLT_bTY",
        "outputId": "4bc70207-a29e-4f6a-e845-d034d6798d7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Running on http://927e-35-204-93-188.ngrok-free.app\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [07/Feb/2024 10:21:20] \"POST /receive_data HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Feb/2024 10:21:32] \"POST /receive_data HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    }
  ]
}